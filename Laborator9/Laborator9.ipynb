{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = magenta> Lixandru Andreaa-Bianca Grupa10LF382\n",
    "<br> \n",
    "<font color = magenta> Pepene Adina-FLorentina Grupa10LF382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele de regresie\n",
    "\n",
    "Folositi urmatoarele seturi de date:\n",
    "1. [CPU Computer Hardware](https://archive.ics.uci.edu/ml/datasets/Computer+Hardware); excludeti din dataset coloanele: vendor name, model name, estimated relative performance; se va estima coloana \"published relative performance\".\n",
    "1. [Boston Housing](http://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n",
    "1. [Wisconsin Breast Cancer](http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html); cautati in panelul din stanga Wisconsin Breast Cancer si urmati pasii din \"My personal Notes\"\n",
    "1. [Communities and Crime](http://archive.ics.uci.edu/ml/datasets/communities+and+crime); stergeti primele 5 dimensiuni si trasaturile cu missing values.\n",
    "\n",
    "Pentru fiecare set de date aplicati minim 5 modele de regresie din scikit learn. Pentru fiecare raportati: mean absolute error, mean squared error, median absolute error - a se vedea [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) - folosind 5 fold cross validation. Valorile hiperparametrilor trebuie cautate cu grid search (cv=3)  si random search (n_iter dat de voi). Metrica folosita pentru cautarea hiperparametrilor va fi mean squared error. Raportati mediile rezultatelor atat pentru fold-urile de antrenare, cat si pentru cele de testare; indicatie: puteti folosi metoda `cross_validate` cu parametrul `return_train_score=True`, iar ca model un obiect de tip `GridSearchCV` sau `RandomizedSearchCV`.\n",
    "\n",
    "Rezultatele vor fi trecute intr-un dataframe. Intr-o stare intermediara, valorile vor fi calculate cu semnul minus: din motive de implementare, biblioteca sklearn transforma scorurile in numere negative; a se vedea imaginea de mai jos:\n",
    "\n",
    "![intermediate report](./images/cpu_intermediate_blurred.png)\n",
    "\n",
    "\n",
    "Valorile vor fi aduse la interval pozitiv, apoi vor fi marcate cele maxime si minime; orientativ, se poate folosi imaginea de mai jos, reprezentand dataframe afisat in notebook; puteti folosi alte variante de styling pe dataframe precum la https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html#.  \n",
    "\n",
    "Se va crea un raport final in format HTML sau PDF - fisier(e) separat(e). Raportul trebuie sa contina minimal: numele setului de date si obiectul dataframe; preferabil sa se pastreze marcajul de culori realizat in notebook.\n",
    "\n",
    "![report](./images/cpu_results_blurred.png)\n",
    "\n",
    "Notare:\n",
    "1. Se acorda 20 de puncte din oficiu.\n",
    "1. Optimizare si cuantificare de performanta a modelelor: 3 puncte pentru fiecare combinatie set de date + model = 60 de puncte\n",
    "1. Documentare modele: numar modele * 2 puncte = 10 puncte. Documentati in jupyter notebook fiecare din modelele folosite, in limba romana. Puteti face o sectiune separata cu documentarea algoritmilor. Fiecare model trebuie sa aiba o descriere de minim 20 de randuri, minim o imagine asociata si minim 2 referinte bibliografice.\n",
    "1. 10 puncte: export in format HTML sau PDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notare:* rezolvarea va fi incarcata pe platforma de elearning in saptamana 11-15 mai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citirea si separarea datelor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>myct</th>\n",
       "      <th>mmin</th>\n",
       "      <th>mmax</th>\n",
       "      <th>cach</th>\n",
       "      <th>chmin</th>\n",
       "      <th>chmax</th>\n",
       "      <th>prp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>256</td>\n",
       "      <td>6000</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   myct  mmin   mmax  cach  chmin  chmax  prp\n",
       "0   125   256   6000   256     16    128  198\n",
       "1    29  8000  32000    32      8     32  269\n",
       "2    29  8000  32000    32      8     32  220\n",
       "3    29  8000  32000    32      8     32  172\n",
       "4    29  8000  16000    32      8     16  132"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_cpu_names = ['vendor_names', 'model', 'myct', 'mmin', 'mmax', 'cach', 'chmin', 'chmax', 'prp', 'erp']\n",
    "\n",
    "dataframe_cpu = pd.read_csv('data\\machine\\machine.data', names=dataframe_cpu_names, header=None)\n",
    "\n",
    "dataframe_cpu = dataframe_cpu.drop(columns=['vendor_names', 'model', 'erp'])\n",
    "\n",
    "dataframe_cpu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_cpu_x = dataframe_cpu.values[:, :-1]\n",
    "dataframe_cpu_y = dataframe_cpu.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad    tax  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   ptratio       b  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_housing_names = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis',\n",
    "                           'rad', 'tax', 'ptratio', 'b', 'lstat', 'medv']\n",
    "\n",
    "dataframe_housing = pd.read_csv('data\\housing\\housing.data', names=dataframe_housing_names, header = None, sep=r\"\\s+\")\n",
    "\n",
    "dataframe_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_housing_x = dataframe_housing.values[:, :-1]\n",
    "dataframe_housing_y = dataframe_housing.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cautarea hiperparametrilor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_grid(model, parameters:dict , x_train:np.ndarray, y_train:np.ndarray, x_test:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    '''This function takes the parameters and search for best parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x_train, y_train - input and output values for model train\n",
    "        x_test, y_test - input and output values for model test\n",
    "    \n",
    "    It returns the errors.\n",
    "    '''\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring='neg_mean_squared_error', cv=3,  return_train_score=True)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    return errors(grid_search, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_random(model, parameters:dict , x_train:np.ndarray, y_train:np.ndarray, x_test:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    '''This function takes the parameters and search for best parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x_train, y_train - input and output values for model train\n",
    "        x_test, y_test - input and output values for model test\n",
    "    \n",
    "    It returns the errors.\n",
    "    '''\n",
    "    rand_search = RandomizedSearchCV(estimator=model, param_distributions=parameters, scoring='neg_mean_squared_error', cv=3,  return_train_score=True)\n",
    "    rand_search.fit(x_train, y_train)\n",
    "    \n",
    "    return errors(rand_search, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcularea erorilor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(model, x_train:np.ndarray, y_train:np.ndarray, \n",
    "           x_test:np.ndarray, y_test:np.ndarray)-> List[float]:\n",
    "    \n",
    "    '''Errors function calculates mean absolute error, \n",
    "    mean squared error and median absolute error for model.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        x_train, y_train - input and output values for model train\n",
    "        x_test, y_test - input and output values for model test\n",
    "        \n",
    "    Returns a list with mean of errors:\n",
    "    neg_mean_absolute_error for train and test data\n",
    "    neg_mean_squared_error for train and test data\n",
    "    neg_median_absolute_error for train and test data\n",
    "    '''\n",
    "    \n",
    "    scores_neg_mean_abs_err_train = (-1)*cross_val_score(model, x_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error').mean()\n",
    "    scores_neg_mean_abs_err_test = (-1)*cross_val_score(model, x_test, y_test, cv = 5, scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "    scores_neg_mean_sq_err_train = (-1)*cross_val_score(model, x_train, y_train, cv = 5, scoring = 'neg_mean_squared_error').mean()\n",
    "    scores_neg_mean_sq_err_test = (-1)*cross_val_score(model, x_test, y_test, cv = 5, scoring = 'neg_mean_squared_error').mean()\n",
    "    \n",
    "    scores_neg_med_abs_err_train = (-1)*cross_val_score(model, x_train, y_train, cv = 5, scoring = 'neg_median_absolute_error').mean()\n",
    "    scores_neg_med_abs_err_test = (-1)*cross_val_score(model, x_test, y_test, cv = 5, scoring = 'neg_median_absolute_error').mean()\n",
    "    \n",
    "    result = [scores_neg_mean_abs_err_train, scores_neg_mean_abs_err_test,\n",
    "              scores_neg_mean_sq_err_train, scores_neg_mean_sq_err_test,\n",
    "              scores_neg_med_abs_err_train, scores_neg_med_abs_err_test]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor Regression(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(model, parameters:dict, x_train:np.ndarray, y_train:np.ndarray, x_test:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    '''KNN function get params and do regression based on k-nearest\n",
    "    neighbors with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x_train, y_train - input and output values for model train\n",
    "        x_test, y_test - input and output values for model test \n",
    "    '''\n",
    "    \n",
    "    print (\"KNN\", search_grid(model, parameters, x_train, y_train, x_test, y_test))\n",
    "    print ()\n",
    "    print (\"KNN\", search_random(model, parameters, x_train, y_train, x_test, y_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(model, parameters:dict, x_train:np.ndarray, y_train:np.ndarray, x_test:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    '''decision_tree function get params and do regression based on k-nearest\n",
    "    neighbors with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x_train, y_train - input and output values for model train\n",
    "        x_test, y_test - input and output values for model test \n",
    "    '''\n",
    "    \n",
    "    print (\"Decision tree\", search_grid(model, parameters, x_train, y_train, x_test, y_test))\n",
    "    print ()\n",
    "    print (\"Decision tree\", search_random(model, parameters, x_train, y_train, x_test, y_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(model, parameters:dict, x_train:np.ndarray, y_train:np.ndarray, x_test:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    '''random_forest function get params and do regression based on k-nearest\n",
    "    neighbors with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x_train, y_train - input and output values for model train\n",
    "        x_test, y_test - input and output values for model test \n",
    "    '''\n",
    "    \n",
    "    print (\"Random Forest\", search_grid(model, parameters, x_train, y_train, x_test, y_test))\n",
    "    print ()\n",
    "    print (\"Random Forest\", search_random(model, parameters, x_train, y_train, x_test, y_test))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicarea modelelor pentru generarea erorilor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_knn(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    model_knn = KNeighborsRegressor()\n",
    "    parameters_knn = {'n_neighbors': [3, 4, 5, 6, 8, 9, 10, 11], 'p': [2, 3, 4, 5]}\n",
    "    KNN(model_knn, parameters_knn, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_decision_tree(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    model_decision_tree = DecisionTreeRegressor()\n",
    "    parameters_decision_tree = {'criterion': ['mse', 'friedman_mse'], 'max_depth': [3, 4, 5, 6, 7, 8]}\n",
    "    decision_tree(model_decision_tree, parameters_decision_tree, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_random_forest(x_train:np.ndarray, x_test:np.ndarray, y_train:np.ndarray, y_test:np.ndarray):\n",
    "    \n",
    "    model_random_forest = RandomForestRegressor()\n",
    "    parameters_random_forest = {'criterion': ['mse', 'friedman_mse'], 'n_estimators': [ 80, 100, 120], 'max_depth': [3, 4, 5, 6]}\n",
    "    random_forest(model_random_forest, parameters_random_forest, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/3)\n",
    "    start_knn(x_train, x_test, y_train, y_test)\n",
    "    start_decision_tree( x_train, x_test, y_train, y_test)\n",
    "    start_random_forest(x_train, x_test, y_train, y_test)\n",
    "    \n",
    "#     model_knn = KNeighborsRegressor()\n",
    "#     model_decision_tree = DecisionTreeRegressor()\n",
    "#     model_random_forest = RandomForestRegressor()\n",
    "\n",
    "#     parameters_knn = {'n_neighbors': [3, 4, 5, 6, 8, 9, 10, 11], 'p': [2, 3, 4, 5]}\n",
    "#     parameters_decision_tree = {'criterion': ['mse', 'friedman_mse'], 'max_depth': [3, 4, 5, 6, 7, 8]}\n",
    "#     parameters_random_forest = {'criterion': ['mse', 'friedman_mse'], 'n_estimators': [ 80, 100, 120, 130, 150], 'max_depth': [3, 4, 5, 6, 7, 8]}\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/3)\n",
    " \n",
    "#     KNN(model_knn, parameters_knn, x_train, y_train, x_test, y_test)\n",
    "#     decision_tree(model_decision_tree, parameters_decision_tree, x_train, y_train, x_test, y_test)\n",
    "#     random_forest(model_random_forest, parameters_random_forest, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN [42.68921957671957, 36.82380952380952, 8767.664162624927, 10614.407936507938, 18.200000000000003, 12.833333333333334]\n",
      "\n",
      "KNN [45.674867724867724, 37.200714285714284, 9077.47800191064, 12918.742876984126, 18.9, 12.833333333333334]\n",
      "Decision tree [45.08236837868669, 40.75997071555894, 11109.543434058976, 26474.49910235083, 16.641284721223247, 17.5682734204793]\n",
      "\n",
      "Decision tree [46.48844252240789, 36.19349232809066, 10639.031023824655, 22313.767270764925, 17.082640817327796, 15.055128205128204]\n",
      "Random Forest [39.578649002027895, 32.57424296819673, 8574.372179956194, 9279.9359416881, 16.8116147137911, 13.884389354638998]\n",
      "\n",
      "Random Forest [38.466451093351694, 32.06806359368971, 8813.539475872623, 8887.910627335466, 14.735785267133291, 13.156209348746462]\n"
     ]
    }
   ],
   "source": [
    "start(dataframe_cpu_x, dataframe_cpu_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN [4.593110513608428, 6.068276391364626, 46.971807609897816, 70.9260322244779, 2.762833333333333, 4.39861111111111]\n",
      "\n",
      "KNN [4.542745585796507, 6.099511685482274, 44.92548227685104, 72.31584053738638, 2.846444444444443, 4.2492777777777775]\n",
      "Decision tree [3.017254193715975, 4.116064996457743, 19.693833418355144, 62.61004511847301, 2.012763954008497, 2.610347358578775]\n",
      "\n",
      "Decision tree [2.897754932078234, 4.792076658744305, 18.682631960773094, 48.57377787575418, 2.103844447822707, 2.7883825027485742]\n",
      "Random Forest [2.41796830412716, 3.05201463204209, 13.891419478767057, 22.26218620487556, 1.7800455496198135, 2.179571847553695]\n",
      "\n",
      "Random Forest [2.408843018433673, 3.1201616091212783, 13.95493839097169, 22.798208018660688, 1.7663121904849952, 2.1468380406748997]\n"
     ]
    }
   ],
   "source": [
    "start(dataframe_housing_x, dataframe_housing_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Nearest Neighbors Regression(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Regression este un algoritm simplu care stochează toate cazurile posibile și prezice o valoare pe baza unei măsuri de similaritate (de exemplu, funcții de distanță). KNN a fost utilizat în estimarea statistică și recunoașterea modelului de la începutul anilor 1970 ca tehnică non-parametrică. Regresia KNN utilizează aceleași funcții de distanță ca și clasificarea KNN. K reprezinta numarul de vecini care este cautat pentru a-l determina pe cel mai potrivit.\n",
    "<br><br>Când __K = 1__ algoritmul este cunoscut ca nearest neighbor. Acesta este cel mai simplu caz. Sa presupunem că P1 este un punct pentru care trebuie sa ii prezicem eticheta. Mai întâi, găsim cel mai apropiat punct de P1 și atribuim eticheta acestuia lui P1.\n",
    "\n",
    "<img src=\"resources/image1.png\" width=\"400\">\n",
    "\n",
    "Un alt exemplu pentru __K = 3__ este:\n",
    "\n",
    "<img src=\"resources/image2.png\" width=\"400\">\n",
    "\n",
    "Urmarind plot-urile de mai jos putem observa ca folosind doar un singur vecin, fiecare punct din setul de antrenare are o influență evidentă asupra predicțiilor, iar valorile prezise trec prin toate punctele. Aceasta duce la o predicție foarte nesigură. Considerând mai mulți vecini se ajunge la predicții mai ușoare, dar acestea nu se potrivesc și cu datele de instruire.\n",
    "\n",
    "<img src=\"resources/image3.png\" width=\"1000\">\n",
    "\n",
    "Regresia K-Nearest Neighbors poate fi utilizată în cazurile în care etichetele de date sunt continue, mai degrabă decât variabile discrete. Eticheta atribuită unui punct de interogare este calculată pe baza mediilor etichetelor celor mai apropiați vecini.\n",
    "În faza de clasificare, k este o constantă definită de utilizator, iar un vector nemarcat (o interogare sau un punct de testare) este clasificat prin atribuirea etichetei care este cea mai frecventă dintre eșantioanele de pregătire k cele mai apropiate de punctul de interogare.\n",
    "\n",
    "__Metode de calcul al distantei dintre doua puncte__\n",
    "\n",
    "Algoritmul KNN este un algoritm de invatare supervizata, care foloseste distante pentru a gasi similitudini si diferente.\n",
    "\n",
    "__1. Distanta Euclidiana:__ este cea mai frecventă și măsoară distanța liniară dintre două probe\n",
    "__2. Distanta Manhattan:__ măsoară timpul de călătorie punct-la-punct și este utilizata în mod obișnuit pentru predictorii binari\n",
    "\n",
    "<img src=\"resources/image4.png\" width=\"250\">\n",
    "\n",
    "__3. Distanta Hamming__\n",
    "\n",
    "<img src=\"resources/image5.png\" width=\"150\">\n",
    "\n",
    "\n",
    "Distanta Euclidiana si distanta Manhattan sunt folosite pentru variabile continue, iar distanta Hamming este folosita pentru variabile categoriale.\n",
    "\n",
    "Cum aflam care este cel mai potrivit K?\n",
    "Daca folosim un K foarte mic vom face overfitting pe setul de antrenare si nu vom avea rezultate deloc bune pentru setul de testare. Daca folosim un K foarte mare nu vom obtine rezultate bune nici pe setul de antrenare, nici pe setul de intrare. Ce facem? Solutia este curba 'elbow curve'. Un K bun poate fi selectat prin diferite tehnici euristice (ex: optimizarea hiperparametrului). \n",
    "\n",
    "<img src=\"resources/image6.png\" width=\"550\">\n",
    "\n",
    "Un dezavantaj al clasificării de bază a „votului majoritar” apare atunci când exemple dintr-o clasă mai frecventă tind să domine predicția noului exemplu, deoarece acestea tind să fie comune printre cei mai apropiați k din cauza numărului mare. O modalitate de a depăși această problemă este ponderea clasificării, ținând cont de distanța de la punctul de testare la fiecare dintre cei mai apropiați k vecini ai săi. Clasa (sau valoarea, în problemele de regresie) din fiecare dintre cele mai apropiate k puncte este înmulțită cu o greutate proporțională cu inversul distanței de la acel punct la punctul de testare. \n",
    "\n",
    "Precizia algoritmului KNN poate fi sever degradată prin prezența unor caracteristici zgomotoase sau irelevante sau dacă scările de caracteristici nu sunt în concordanță cu importanța lor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie:\n",
    "\n",
    "1. https://www.saedsayad.com/k_nearest_neighbors_reg.htm\n",
    "2. https://medium.com/analytics-vidhya/k-neighbors-regression-analysis-in-python-61532d56d8e4\n",
    "3. https://scikit-learn.org/stable/modules/neighbors.html#regression\n",
    "4. https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/\n",
    "5. https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arborele decizional construiește modele de regresie sau clasificare sub forma unei structuri de arbori. Acesta descompune un set de date în subseturi mai mici și mai mici, în același timp se dezvoltă treptat un arbore de decizie asociat. Rezultatul final este un arbore cu noduri de decizie și noduri frunze. Un nod de decizie are două sau mai multe ramuri (de exemplu, însorit, înnorat și ploios), fiecare reprezentând valori pentru atributul testat. Nodul frunza (de exemplu, ore jucate) reprezintă o decizie cu privire la ținta numerică. Cel mai înalt nod decizie într-un arbore corespunde celui mai bun predictor numit nod rădăcină. Arborii de decizie pot gestiona atât date categorice, cât și numerice. Nodurile de decizie reprezinta conditiile, iar nodurile frunza reprezinta rezultatele.\n",
    "\n",
    "<br>\n",
    "Exemplul de mai jos care arată un arbore de decizie care determina cel mai mic numar dintre trei elemente.\n",
    "\n",
    "<img src=\"resources/image8.jpg\" width=\"650\">\n",
    "\n",
    "Regresia arborelui decizional observă trasaturile unui obiect și ataseaza în structura unui arbore trasaturile pentru a prezice date în viitor. Ieșirea continuă înseamnă că ieșirea / rezultatul nu este discret, adică nu este reprezentat doar de un set discret, cunoscut de numere sau valori.\n",
    "\n",
    "_Exemplu de ieșire discreta:_ un model de predicție a vremii care prevede dacă va fi sau nu ploaie într-o anumită zi.\n",
    "<br>_Exemplu de ieșire continuă:_ model de predicție a profitului care afirmă profitul probabil care poate fi generat din vânzarea unui produs.\n",
    "\n",
    "Pornind de la rădăcină, datele sunt împărțite pe caracteristicile care au ca rezultat cel mai mare câștig de informații (IG- Information Gain). Într-un proces iterativ, repetăm această procedură de divizare la fiecare nod copil până când frunzele sunt pure - adică eșantioanele la fiecare nod aparțin aceleiași clase.\n",
    "\n",
    "În practică, acest lucru poate duce la un arbore foarte adânc, cu numeroase noduri, ceea ce poate duce cu ușurință la supraîncadrare. Astfel, de obicei dorim să tăiem copacul prin stabilirea unei limite pentru adâncimea maximă a copacului.\n",
    "\n",
    "Pentru a împărți nodurile in cele mai relevante caracteristici, trebuie să definim o funcție obiectivă pe care dorim să o optimizăm prin algoritmul de învățare. Aici, funcția noastră obiectivă este de a maximiza câștigul de informații la fiecare divizare, pe care o definim astfel:\n",
    "\n",
    "<img src=\"resources/image9.png\" width=\"500\">\n",
    "\n",
    "Pentru a utiliza un arbore de decizie pentru regresie, avem nevoie de o măsurătoare care să fie potrivită pentru variabile continue, astfel încât să definim măsura de impuritate folosind eroarea medie pătrată (MSE) a nodurilor copiilor:\n",
    "\n",
    "<img src=\"resources/image10.png\" width=\"250\">\n",
    "\n",
    "Aici, Nt este numărul de eșantioane de antrenament al nodului t, Dt este subsetul de formare al nodul t, y (i) este adevărata valoare și ŷt este valoarea țintă prevăzută:\n",
    "\n",
    "<img src=\"resources/image11.png\" width=\"125\">\n",
    "\n",
    "În practică, este important să stii cum să alegi o valoare adecvată pentru o adâncime a unui arbore pentru a face overfit sau a face undefit. Să știi cum să combini arbori de decizie pentru a forma o pădure aleatoare este, de asemenea, util, intrucat de obicei, are o performanță de generalizare mai bună decât un arbore de decizie individual, ceea ce ajută la scăderea variației modelului. De asemenea, este mai puțin sensibil la valorile din setul de date și nu necesită prea multe reglări de parametri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie\n",
    "\n",
    "1. https://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/\n",
    "2. https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054 -> foarte relevant\n",
    "3. https://saedsayad.com/decision_tree_reg.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest este o tehnică de ansamblu capabilă să îndeplinească atât sarcini de regresie, cât și de clasificare cu utilizarea mai multor arbori de decizie și o tehnică numită Bootstrap Aggregation, cunoscută de obicei ca bagging. Bagging, în metoda Random Forest, implică instruirea fiecărui arbore de decizie pe un eșantion de date diferit, unde sampling-ul se face cu înlocuire.\n",
    "\n",
    "Arborii de decizie sunt o metodă populară pentru diferite metode de machine learning.\n",
    "În special, arborii care sunt foarte adânci tind să învețe tipare extrem de neregulate: își potrivesc seturile de antrenament ai auvariație foarte mare. Random Forest este o modalitate de medie a mai multor arbori de decizie profundă, instruiți pe diferite părți ale aceluiași set de antrenament, cu scopul de a reduce variația. \n",
    "\n",
    "Procedura de bootstrapping duce la o performanță mai bună a modelului, deoarece scade variația modelului, fără a crește bias-urile. Aceasta înseamnă că, deși iesirile unui singur arbore sunt foarte sensibile la zgomot în setul său de formare, media mai multor arbori nu este, atâta timp cât nu exista corelații. Pur și simplu antrenarea multor arbori pe un singur set de antrenament ar oferi arbori puternic corelați (sau chiar același arbore de multe ori, dacă algoritmul de antrenare este determinist); eșantionarea bootstrap este o modalitate de a corela copacii arătându-le diferite seturi de antrenament.\n",
    "\n",
    "<img src=\"resources/image12.jpg\" width=\"400\">\n",
    "\n",
    "Metoda de mai sus descrie algoritmul de bagaj original pentru arbori. Random Forest diferă într-un singur mod de această schemă generală: folosesc un algoritm de învățare arbore modificat care selectează, la fiecare împărțire a candidatului în procesul de învățare, un subset aleator al caracteristicilor. Acest proces este uneori numit „bagaj de funcții”(feature bagging). Motivul pentru a face acest lucru este corelația arborilor dintr-un eșantion de bootstrap obișnuit: dacă una sau câteva caracteristici sunt predictori foarte puternici pentru variabila de răspuns(ieșire țintă), aceste caracteristici vor fi selectate în multe dintre arborele B, cauzându-le a deveni corelat. O analiză a modului în care bagajul și proiecția aleatorie a subspațiului contribuie la câștigurile de precizie în diferite condiții.\n",
    "\n",
    "Diferite tipuri de modele au avantaje diferite. Modelul Random Forest este foarte bun în tratarea datelor tabulare cu caracteristici numerice sau caracteristici categorice cu mai puțin de sute de categorii. Spre deosebire de modelele liniare, Random Forest este capabil să capteze interacțiuni neliniare între caracteristici și țintă.\n",
    "\n",
    "O notă importantă este că modelele bazate pe arbori nu sunt proiectate să funcționeze cu caracteristici foarte rare. Atunci când avem de a face cu date de intrare reduse(de exemplu, caracteristici categorice cu dimensiuni mari), putem prelucra în prealabil funcțiile rare pentru a genera statistici numerice, sau putem trece la un model liniar, care este mai potrivit pentru astfel de scenarii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie\n",
    "\n",
    "1. https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb\n",
    "2. https://en.wikipedia.org/wiki/Random_forest\n",
    "3. https://turi.com/learn/userguide/supervised-learning/random_forest_regression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
