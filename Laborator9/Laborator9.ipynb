{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = magenta> Lixandru Andreaa-Bianca Grupa10LF382\n",
    "<br> \n",
    "<font color = magenta> Pepene Adina-FLorentina Grupa10LF382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele de regresie\n",
    "\n",
    "Folositi urmatoarele seturi de date:\n",
    "1. [CPU Computer Hardware](https://archive.ics.uci.edu/ml/datasets/Computer+Hardware); excludeti din dataset coloanele: vendor name, model name, estimated relative performance; se va estima coloana \"published relative performance\".\n",
    "1. [Boston Housing](http://archive.ics.uci.edu/ml/machine-learning-databases/housing/)\n",
    "1. [Wisconsin Breast Cancer](http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html); cautati in panelul din stanga Wisconsin Breast Cancer si urmati pasii din \"My personal Notes\"\n",
    "1. [Communities and Crime](http://archive.ics.uci.edu/ml/datasets/communities+and+crime); stergeti primele 5 dimensiuni si trasaturile cu missing values.\n",
    "\n",
    "Pentru fiecare set de date aplicati minim 5 modele de regresie din scikit learn. Pentru fiecare raportati: mean absolute error, mean squared error, median absolute error - a se vedea [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) - folosind 5 fold cross validation. Valorile hiperparametrilor trebuie cautate cu grid search (cv=3)  si random search (n_iter dat de voi). Metrica folosita pentru cautarea hiperparametrilor va fi mean squared error. Raportati mediile rezultatelor atat pentru fold-urile de antrenare, cat si pentru cele de testare; indicatie: puteti folosi metoda `cross_validate` cu parametrul `return_train_score=True`, iar ca model un obiect de tip `GridSearchCV` sau `RandomizedSearchCV`.\n",
    "\n",
    "Rezultatele vor fi trecute intr-un dataframe. Intr-o stare intermediara, valorile vor fi calculate cu semnul minus: din motive de implementare, biblioteca sklearn transforma scorurile in numere negative; a se vedea imaginea de mai jos:\n",
    "\n",
    "![intermediate report](./images/cpu_intermediate_blurred.png)\n",
    "\n",
    "\n",
    "Valorile vor fi aduse la interval pozitiv, apoi vor fi marcate cele maxime si minime; orientativ, se poate folosi imaginea de mai jos, reprezentand dataframe afisat in notebook; puteti folosi alte variante de styling pe dataframe precum la https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html#.  \n",
    "\n",
    "Se va crea un raport final in format HTML sau PDF - fisier(e) separat(e). Raportul trebuie sa contina minimal: numele setului de date si obiectul dataframe; preferabil sa se pastreze marcajul de culori realizat in notebook.\n",
    "\n",
    "![report](./images/cpu_results_blurred.png)\n",
    "\n",
    "Notare:\n",
    "1. Se acorda 20 de puncte din oficiu.\n",
    "1. Optimizare si cuantificare de performanta a modelelor: 3 puncte pentru fiecare combinatie set de date + model = 60 de puncte\n",
    "1. Documentare modele: numar modele * 2 puncte = 10 puncte. Documentati in jupyter notebook fiecare din modelele folosite, in limba romana. Puteti face o sectiune separata cu documentarea algoritmilor. Fiecare model trebuie sa aiba o descriere de minim 20 de randuri, minim o imagine asociata si minim 2 referinte bibliografice.\n",
    "1. 10 puncte: export in format HTML sau PDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notare:* rezolvarea va fi incarcata pe platforma de elearning in saptamana 11-15 mai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citirea si separarea datelor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from statistics import mean\n",
    "\n",
    "print('Pandas version:', pd.__version__) #Pandas version: 0.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>myct</th>\n",
       "      <th>mmin</th>\n",
       "      <th>mmax</th>\n",
       "      <th>cach</th>\n",
       "      <th>chmin</th>\n",
       "      <th>chmax</th>\n",
       "      <th>prp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>256</td>\n",
       "      <td>6000</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>32000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>8000</td>\n",
       "      <td>16000</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   myct  mmin   mmax  cach  chmin  chmax  prp\n",
       "0   125   256   6000   256     16    128  198\n",
       "1    29  8000  32000    32      8     32  269\n",
       "2    29  8000  32000    32      8     32  220\n",
       "3    29  8000  32000    32      8     32  172\n",
       "4    29  8000  16000    32      8     16  132"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_names = ['vendor_names', 'model', 'myct', 'mmin', 'mmax', 'cach', 'chmin', 'chmax', 'prp', 'erp']\n",
    "\n",
    "cpu = pd.read_csv('data\\machine\\machine.data', names = cpu_names, header = None)\n",
    "\n",
    "cpu = cpu.drop(columns = ['vendor_names', 'model', 'erp'])\n",
    "\n",
    "cpu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_x = cpu.values[:, :-1]\n",
    "cpu_y = cpu.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad    tax  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   ptratio       b  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_names = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis',\n",
    "                           'rad', 'tax', 'ptratio', 'b', 'lstat', 'medv']\n",
    "\n",
    "housing = pd.read_csv('data\\housing\\housing.data', names = housing_names, header = None, sep = r\"\\s+\")\n",
    "\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_x = housing.values[:, :-1]\n",
    "housing_y = housing.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lymph_node</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave_points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave_points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Tumor_size</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>18.02</td>\n",
       "      <td>27.60</td>\n",
       "      <td>117.50</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.09489</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1086</td>\n",
       "      <td>0.07055</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>...</td>\n",
       "      <td>139.70</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.08113</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>3.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>21.37</td>\n",
       "      <td>17.44</td>\n",
       "      <td>137.50</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>0.08836</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.08180</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>...</td>\n",
       "      <td>159.10</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.3449</td>\n",
       "      <td>0.3414</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.09067</td>\n",
       "      <td>2.5</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.2839</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>3.5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lymph_node  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           5        18.02         27.60          117.50     1013.0   \n",
       "1           2        17.99         10.38          122.80     1001.0   \n",
       "2           0        21.37         17.44          137.50     1373.0   \n",
       "3           0        11.42         20.38           77.58      386.1   \n",
       "4           0        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
       "0          0.09489            0.1036          0.1086              0.07055   \n",
       "1          0.11840            0.2776          0.3001              0.14710   \n",
       "2          0.08836            0.1189          0.1255              0.08180   \n",
       "3          0.14250            0.2839          0.2414              0.10520   \n",
       "4          0.10030            0.1328          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0         0.1865  ...           139.70      1436.0            0.1195   \n",
       "1         0.2419  ...           184.60      2019.0            0.1622   \n",
       "2         0.2333  ...           159.10      1949.0            0.1188   \n",
       "3         0.2597  ...            98.87       567.7            0.2098   \n",
       "4         0.1809  ...           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave_points_worst  symmetry_worst  \\\n",
       "0             0.1926           0.3140                0.1170          0.2677   \n",
       "1             0.6656           0.7119                0.2654          0.4601   \n",
       "2             0.3449           0.3414                0.2032          0.4334   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Tumor_size  Time  \n",
       "0                  0.08113         5.0    31  \n",
       "1                  0.11890         3.0    61  \n",
       "2                  0.09067         2.5   116  \n",
       "3                  0.17300         2.0   123  \n",
       "4                  0.07678         3.5    27  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_names = ['Lymph_node', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', \n",
    "                          'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',\n",
    "                          'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se',\n",
    "                          'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave_points_se',\n",
    "                          'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst',\n",
    "                          'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
    "                          'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst' ,'Tumor_size','Time']\n",
    "\n",
    "breast = pd.read_csv('data/breast_cancer/r_wpbc.data', names = breast_names)\n",
    "\n",
    "breast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_x = breast.values[:, :-1]\n",
    "breast_y = breast.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.19           0.33          0.02          0.90          0.12   \n",
       "1        0.00           0.16          0.12          0.74          0.45   \n",
       "2        0.00           0.42          0.49          0.56          0.17   \n",
       "3        0.04           0.77          1.00          0.08          0.12   \n",
       "4        0.01           0.55          0.02          0.95          0.09   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  ...  \\\n",
       "0         0.17         0.34         0.47         0.29        0.32  ...   \n",
       "1         0.07         0.26         0.59         0.35        0.27  ...   \n",
       "2         0.04         0.39         0.47         0.28        0.32  ...   \n",
       "3         0.10         0.51         0.50         0.34        0.21  ...   \n",
       "4         0.05         0.38         0.38         0.23        0.36  ...   \n",
       "\n",
       "   PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "0            0.12              0.42            0.50           0.51   \n",
       "1            0.21              0.50            0.34           0.60   \n",
       "2            0.14              0.49            0.54           0.67   \n",
       "3            0.19              0.30            0.73           0.64   \n",
       "4            0.11              0.72            0.64           0.61   \n",
       "\n",
       "   PctSameState85  LandArea  PopDens  PctUsePubTrans  LemasPctOfficDrugUn  \\\n",
       "0            0.64      0.12     0.26            0.20                 0.32   \n",
       "1            0.52      0.02     0.12            0.45                 0.00   \n",
       "2            0.56      0.01     0.21            0.02                 0.00   \n",
       "3            0.65      0.02     0.39            0.28                 0.00   \n",
       "4            0.53      0.04     0.09            0.02                 0.00   \n",
       "\n",
       "   ViolentCrimesPerPop  \n",
       "0                 0.20  \n",
       "1                 0.67  \n",
       "2                 0.43  \n",
       "3                 0.12  \n",
       "4                 0.03  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_names = ['state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize',\n",
    "                               'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', \n",
    "                               'agePct12t29', 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome',\n",
    "                               'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire',\n",
    "                               'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap',\n",
    "                               'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade',\n",
    "                               'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu',\n",
    "                               'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', \n",
    "                               'MalePctNevMarr', 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', \n",
    "                               'PctKids2Par', 'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids',\n",
    "                               'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5',\n",
    "                               'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8',\n",
    "                               'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam',\n",
    "                               'PctLargHouseOccup', 'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous',\n",
    "                               'PctPersOwnOccup', 'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant',\n",
    "                               'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt',\n",
    "                               'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart',\n",
    "                               'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc', \n",
    "                               'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', \n",
    "                               'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85', 'PctSameCity85', \n",
    "                               'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps', \n",
    "                               'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic',\n",
    "                               'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp',\n",
    "                               'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz',\n",
    "                               'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', \n",
    "                               'PolicOperBudg', 'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', \n",
    "                               'PolicBudgPerPop', 'ViolentCrimesPerPop']\n",
    "\n",
    "communities = pd.read_csv('data\\communities\\communities.data', names = communities_names)\n",
    "\n",
    "communities = communities.drop(columns = ['state', 'county', 'community', 'communityname', 'fold'])\n",
    "communities = communities.replace('?', np.nan)\n",
    "communities = communities.dropna(axis = 1)\n",
    "\n",
    "communities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_x = communities.values[:, :-1]\n",
    "communities_y = communities.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cautarea hiperparametrilor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_grid(model, parameters:dict, x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    '''This function takes the parameters and search for best parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x, y - input and output values\n",
    "    \n",
    "    It returns the errors.\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 1/3)\n",
    "    \n",
    "    grid_search = Pipeline([('scaler', MinMaxScaler()), \n",
    "                            ('search', GridSearchCV(estimator = model, param_grid = parameters, \n",
    "                            scoring = 'neg_mean_squared_error',cv = 3, return_train_score = True))])\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    return errors(grid_search, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_random(model, parameters:dict, x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    '''This function takes the parameters and search for best parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x, y - input and output values\n",
    "    \n",
    "    It returns the errors.\n",
    "    '''\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 1/3)\n",
    "\n",
    "    rand_search = Pipeline([('scaler', MinMaxScaler()), \n",
    "                            ('search', RandomizedSearchCV(estimator = model, param_distributions = parameters, \n",
    "                            scoring = 'neg_mean_squared_error', cv = 3, return_train_score = True))])\n",
    "    rand_search.fit(x_train, y_train)\n",
    "    \n",
    "    return errors(rand_search, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcularea erorilor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(model, x:np.ndarray, y:np.ndarray)-> List[float]:\n",
    "    \n",
    "    '''Errors function calculates mean absolute error, \n",
    "    mean squared error and median absolute error for model.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        x, y - input and output values\n",
    "        \n",
    "    Returns a list with mean of errors:\n",
    "    neg_mean_absolute_error for train and test data\n",
    "    neg_mean_squared_error for train and test data\n",
    "    neg_median_absolute_error for train and test data\n",
    "    '''\n",
    "    \n",
    "    scores = cross_validate(model, x, y, cv = 5, scoring = ['neg_mean_absolute_error',\n",
    "                                                            'neg_mean_squared_error',\n",
    "                                                            'neg_median_absolute_error'], return_train_score = True)\n",
    "    \n",
    "    result = [(-1)*scores['train_neg_mean_absolute_error'].mean(), (-1)*scores['train_neg_mean_squared_error'].mean(),\n",
    "              (-1)*scores['train_neg_median_absolute_error'].mean(), (-1)*scores['test_neg_mean_absolute_error'].mean(),\n",
    "              (-1)*scores['test_neg_mean_squared_error'].mean(), (-1)*scores['test_neg_median_absolute_error'].mean(),\n",
    "              scores['fit_time'].mean(), scores['score_time'].mean()]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor Regression(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(model, parameters:dict, x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    '''KNN function get params and do regression based on k-nearest\n",
    "    neighbors with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x, y - input and output values\n",
    "    '''\n",
    "    \n",
    "    main_list.append(search_grid(model, parameters, x, y))\n",
    "    main_list.append(search_random(model, parameters, x, y))\n",
    "    intermediar_list.append(['KNN', 'GridSearchCV'])\n",
    "    intermediar_list.append(['KNN', 'RandomizedSearchCV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(model, parameters:dict, x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    '''decision_tree function get params and do regression \n",
    "    with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x, y - input and output values\n",
    "    '''\n",
    "    \n",
    "    main_list.append(search_grid(model, parameters, x, y))\n",
    "    main_list.append(search_random(model, parameters, x, y))\n",
    "    intermediar_list.append(['Decision tree', 'GridSearchCV'])\n",
    "    intermediar_list.append(['Decision tree', 'RandomizedSearchCV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(model, parameters:dict, x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    '''random_forest function get params and do regression \n",
    "    with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x, y - input and output values\n",
    "    '''\n",
    "    \n",
    "    main_list.append(search_grid(model, parameters, x, y))\n",
    "    main_list.append(search_random(model, parameters, x, y))\n",
    "    intermediar_list.append(['Random Forest', 'GridSearchCV'])\n",
    "    intermediar_list.append(['Random Forest', 'RandomizedSearchCV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr(model, parameters:dict, x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    '''svr function get params and do regression\n",
    "    with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x, y - input and output values\n",
    "    '''\n",
    "    \n",
    "    main_list.append(search_grid(model, parameters, x, y))\n",
    "    main_list.append(search_random(model, parameters, x, y))\n",
    "    intermediar_list.append(['SVR', 'GridSearchCV'])\n",
    "    intermediar_list.append(['SVR', 'RandomizedSearchCV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process(model, parameters:dict, x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    '''gaussian_process function get params and do regression \n",
    "    with best parameters generated by grid search and random search.\n",
    "    It generates the error values.\n",
    "    \n",
    "    Parameters:\n",
    "        model - model from sklearn \n",
    "        parameters - all the alternatives to the hyperparameters\n",
    "        x, y - input and output values\n",
    "    '''\n",
    "    \n",
    "    main_list.append(search_grid(model, parameters, x, y))\n",
    "    main_list.append(search_random(model, parameters, x, y))\n",
    "    intermediar_list.append(['Gaussian process', 'GridSearchCV'])\n",
    "    intermediar_list.append(['Gaussian process', 'RandomizedSearchCV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicarea modelelor pentru generarea erorilor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_knn(x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    model_knn = KNeighborsRegressor()\n",
    "    parameters_knn = {'n_neighbors': [3, 4, 5, 6, 8, 9, 10, 11], 'p': [2, 3, 4, 5]}\n",
    "    KNN(model_knn, parameters_knn, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_decision_tree(x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    model_decision_tree = DecisionTreeRegressor()\n",
    "    parameters_decision_tree = {'criterion': ['mse', 'friedman_mse'], 'max_depth': [3, 4, 5, 6, 7, 8]}\n",
    "    decision_tree(model_decision_tree, parameters_decision_tree, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_random_forest(x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    model_random_forest = RandomForestRegressor()\n",
    "    parameters_random_forest = {'criterion': ['mse', 'friedman_mse'], 'n_estimators': [80, 100, 120], 'max_depth': [3, 4, 5, 6]}\n",
    "    random_forest(model_random_forest, parameters_random_forest, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_svr(x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    model_svr = SVR()\n",
    "    parameters_svr = {'kernel': ['linear', 'rbf', 'poly'], 'gamma': [0.1, 1], 'C': [0.1, 1, 10, 100]}\n",
    "    svr(model_svr, parameters_svr, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_gaussian_process(x:np.ndarray, y:np.ndarray):\n",
    "    \n",
    "    model_gaussian_process = GaussianProcessRegressor()\n",
    "    parameters_gaussian_process = {'n_restarts_optimizer': [1, 5, 7, 13], 'alpha': [1e-10, 1e-4, 1e-2]}\n",
    "    gaussian_process(model_gaussian_process, parameters_gaussian_process, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "names:list = ['train_mean_absolute_error', 'train_mean_squared_error', 'train_median_absolute_error', \n",
    "              'test_mean_absolute_error', 'test_mean_squared_error', 'test_median_absolute_error', \n",
    "              'fit_time', 'score_time']\n",
    "    \n",
    "main_list:list = []\n",
    "intermediar_list:list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorarea valorilor maxime si minime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series red.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    \n",
    "    return ['background-color: red' if v else '' for v in is_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the minimum in a Series green.\n",
    "    '''\n",
    "    is_min = s == s.min()\n",
    "    \n",
    "    return ['background-color: green' if v else '' for v in is_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(x:np.ndarray, y:np.ndarray, name:str):\n",
    "    \n",
    "    main_list.clear()\n",
    "    intermediar_list.clear()\n",
    "    \n",
    "    start_knn(x, y)\n",
    "    start_decision_tree(x, y)\n",
    "    start_random_forest(x, y)\n",
    "    start_svr(x, y)\n",
    "    start_gaussian_process(x, y)\n",
    "    \n",
    "    data_frame = pd.DataFrame(main_list, columns = names)\n",
    "    intermediar_data_frame = pd.DataFrame(intermediar_list, columns = ['model_name', 'search_strategy'])\n",
    "    intermediar_data_frame.style\n",
    "\n",
    "    result_data_frame = pd.concat([intermediar_data_frame, data_frame], axis = 1)\n",
    "    result_data_frame.style.apply(highlight_max, \n",
    "                               subset = pd.IndexSlice[:, names]).apply(highlight_min, \n",
    "                                                                       subset = pd.IndexSlice[:, names])\n",
    "    \n",
    "    result = pd.DataFrame(result_data_frame, columns=['model_name', 'search_strategy']+names)\n",
    "    result = result.style.apply(highlight_max, subset= pd.IndexSlice[:, names]).apply(highlight_min,\n",
    "                                                                                              subset= pd.IndexSlice[:, names])\n",
    "    \n",
    "    f=open(f\"{name}.html\",\"w\")\n",
    "    f.write(result.render())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start(cpu_x, cpu_y, \"CPU Computer Hardware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start(housing_x, housing_y, \"Housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start(breast_x, breast_y, \"Breast Cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start(communities_x, communities_y, \"Communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Nearest Neighbors Regression(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Regression este un algoritm simplu care stochează toate cazurile posibile și prezice o valoare pe baza unei măsuri de similaritate (de exemplu, funcții de distanță). KNN a fost utilizat în estimarea statistică și recunoașterea modelului de la începutul anilor 1970 ca tehnică non-parametrică. Regresia KNN utilizează aceleași funcții de distanță ca și clasificarea KNN. K reprezinta numarul de vecini care este cautat pentru a-l determina pe cel mai potrivit.\n",
    "<br><br>Când __K = 1__ algoritmul este cunoscut ca nearest neighbor. Acesta este cel mai simplu caz. Sa presupunem că P1 este un punct pentru care trebuie sa ii prezicem eticheta. Mai întâi, găsim cel mai apropiat punct de P1 și atribuim eticheta acestuia lui P1.\n",
    "\n",
    "<img src=\"resources/image1.png\" width=\"400\">\n",
    "\n",
    "Un alt exemplu pentru __K = 3__ este:\n",
    "\n",
    "<img src=\"resources/image2.png\" width=\"400\">\n",
    "\n",
    "Urmarind plot-urile de mai jos putem observa ca folosind doar un singur vecin, fiecare punct din setul de antrenare are o influență evidentă asupra predicțiilor, iar valorile prezise trec prin toate punctele. Aceasta duce la o predicție foarte nesigură. Considerând mai mulți vecini se ajunge la predicții mai ușoare, dar acestea nu se potrivesc și cu datele de instruire.\n",
    "\n",
    "<img src=\"resources/image3.png\" width=\"1000\">\n",
    "\n",
    "Regresia K-Nearest Neighbors poate fi utilizată în cazurile în care etichetele de date sunt continue, mai degrabă decât variabile discrete. Eticheta atribuită unui punct de interogare este calculată pe baza mediilor etichetelor celor mai apropiați vecini.\n",
    "În faza de clasificare, k este o constantă definită de utilizator, iar un vector nemarcat (o interogare sau un punct de testare) este clasificat prin atribuirea etichetei care este cea mai frecventă dintre eșantioanele de pregătire k cele mai apropiate de punctul de interogare.\n",
    "\n",
    "__Metode de calcul al distantei dintre doua puncte__\n",
    "\n",
    "Algoritmul KNN este un algoritm de invatare supervizata, care foloseste distante pentru a gasi similitudini si diferente.\n",
    "\n",
    "__1. Distanta Euclidiana:__ este cea mai frecventă și măsoară distanța liniară dintre două probe\n",
    "<br>\n",
    "__2. Distanta Manhattan:__ măsoară timpul de călătorie punct-la-punct și este utilizata în mod obișnuit pentru predictorii binari\n",
    "\n",
    "<img src=\"resources/image4.png\" width=\"250\">\n",
    "\n",
    "__3. Distanta Hamming__\n",
    "\n",
    "<img src=\"resources/image5.png\" width=\"150\">\n",
    "\n",
    "\n",
    "Distanta Euclidiana si distanta Manhattan sunt folosite pentru variabile continue, iar distanta Hamming este folosita pentru variabile categoriale.\n",
    "\n",
    "Cum aflam care este cel mai potrivit K?\n",
    "Daca folosim un K foarte mic vom face overfitting pe setul de antrenare si nu vom avea rezultate deloc bune pentru setul de testare. Daca folosim un K foarte mare nu vom obtine rezultate bune nici pe setul de antrenare, nici pe setul de intrare. Ce facem? Solutia este curba 'elbow curve'. Un K bun poate fi selectat prin diferite tehnici euristice (ex: optimizarea hiperparametrului). \n",
    "\n",
    "<img src=\"resources/image6.png\" width=\"550\">\n",
    "\n",
    "Un dezavantaj al clasificării de bază a „votului majoritar” apare atunci când exemple dintr-o clasă mai frecventă tind să domine predicția noului exemplu, deoarece acestea tind să fie comune printre cei mai apropiați k din cauza numărului mare. O modalitate de a depăși această problemă este ponderea clasificării, ținând cont de distanța de la punctul de testare la fiecare dintre cei mai apropiați k vecini ai săi. Clasa (sau valoarea, în problemele de regresie) din fiecare dintre cele mai apropiate k puncte este înmulțită cu o greutate proporțională cu inversul distanței de la acel punct la punctul de testare. \n",
    "\n",
    "Precizia algoritmului KNN poate fi sever degradată prin prezența unor caracteristici zgomotoase sau irelevante sau dacă scările de caracteristici nu sunt în concordanță cu importanța lor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie:\n",
    "\n",
    "1. https://www.saedsayad.com/k_nearest_neighbors_reg.htm\n",
    "2. https://medium.com/analytics-vidhya/k-neighbors-regression-analysis-in-python-61532d56d8e4\n",
    "3. https://scikit-learn.org/stable/modules/neighbors.html#regression\n",
    "4. https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/\n",
    "5. https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arborele decizional construiește modele de regresie sau clasificare sub forma unei structuri de arbori. Acesta descompune un set de date în subseturi mai mici și mai mici, în același timp se dezvoltă treptat un arbore de decizie asociat. Rezultatul final este un arbore cu noduri de decizie și noduri frunze. Un nod de decizie are două sau mai multe ramuri (de exemplu, însorit, înnorat și ploios), fiecare reprezentând valori pentru atributul testat. Nodul frunza (de exemplu, ore jucate) reprezintă o decizie cu privire la ținta numerică. Cel mai înalt nod decizie într-un arbore corespunde celui mai bun predictor numit nod rădăcină. Arborii de decizie pot gestiona atât date categorice, cât și numerice. Nodurile de decizie reprezinta conditiile, iar nodurile frunza reprezinta rezultatele.\n",
    "\n",
    "<br>\n",
    "Exemplul de mai jos care arată un arbore de decizie care determina cel mai mic numar dintre trei elemente.\n",
    "\n",
    "<img src=\"resources/image8.jpg\" width=\"650\">\n",
    "\n",
    "Regresia arborelui decizional observă trasaturile unui obiect și ataseaza în structura unui arbore trasaturile pentru a prezice date în viitor. Ieșirea continuă înseamnă că ieșirea / rezultatul nu este discret, adică nu este reprezentat doar de un set discret, cunoscut de numere sau valori.\n",
    "\n",
    "_Exemplu de ieșire discreta:_ un model de predicție a vremii care prevede dacă va fi sau nu ploaie într-o anumită zi.\n",
    "<br>_Exemplu de ieșire continuă:_ model de predicție a profitului care afirmă profitul probabil care poate fi generat din vânzarea unui produs.\n",
    "\n",
    "Pornind de la rădăcină, datele sunt împărțite pe caracteristicile care au ca rezultat cel mai mare câștig de informații (IG- Information Gain). Într-un proces iterativ, repetăm această procedură de divizare la fiecare nod copil până când frunzele sunt pure - adică eșantioanele la fiecare nod aparțin aceleiași clase.\n",
    "\n",
    "În practică, acest lucru poate duce la un arbore foarte adânc, cu numeroase noduri, ceea ce poate duce cu ușurință la supraîncadrare. Astfel, de obicei dorim să tăiem copacul prin stabilirea unei limite pentru adâncimea maximă a copacului.\n",
    "\n",
    "Pentru a împărți nodurile in cele mai relevante caracteristici, trebuie să definim o funcție obiectivă pe care dorim să o optimizăm prin algoritmul de învățare. Aici, funcția noastră obiectivă este de a maximiza câștigul de informații la fiecare divizare, pe care o definim astfel:\n",
    "\n",
    "<img src=\"resources/image9.png\" width=\"500\">\n",
    "\n",
    "Pentru a utiliza un arbore de decizie pentru regresie, avem nevoie de o măsurătoare care să fie potrivită pentru variabile continue, astfel încât să definim măsura de impuritate folosind eroarea medie pătrată (MSE) a nodurilor copiilor:\n",
    "\n",
    "<img src=\"resources/image10.png\" width=\"250\">\n",
    "\n",
    "Aici, Nt este numărul de eșantioane de antrenament al nodului t, Dt este subsetul de formare al nodul t, y (i) este adevărata valoare și ŷt este valoarea țintă prevăzută:\n",
    "\n",
    "<img src=\"resources/image11.png\" width=\"125\">\n",
    "\n",
    "În practică, este important să stii cum să alegi o valoare adecvată pentru o adâncime a unui arbore pentru a face overfit sau a face undefit. Să știi cum să combini arbori de decizie pentru a forma o pădure aleatoare este, de asemenea, util, intrucat de obicei, are o performanță de generalizare mai bună decât un arbore de decizie individual, ceea ce ajută la scăderea variației modelului. De asemenea, este mai puțin sensibil la valorile din setul de date și nu necesită prea multe reglări de parametri."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie:\n",
    "\n",
    "1. https://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/\n",
    "2. https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054 -> foarte relevant\n",
    "3. https://saedsayad.com/decision_tree_reg.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest este o tehnică de ansamblu capabilă să îndeplinească atât sarcini de regresie, cât și de clasificare cu utilizarea mai multor arbori de decizie și o tehnică numită Bootstrap Aggregation, cunoscută de obicei ca bagging. Bagging, în metoda Random Forest, implică instruirea fiecărui arbore de decizie pe un eșantion de date diferit, unde sampling-ul se face cu înlocuire.\n",
    "\n",
    "Arborii de decizie sunt o metodă populară pentru diferite metode de machine learning.\n",
    "În special, arborii care sunt foarte adânci tind să învețe tipare extrem de neregulate: își potrivesc seturile de antrenament ai auvariație foarte mare. Random Forest este o modalitate de medie a mai multor arbori de decizie profundă, instruiți pe diferite părți ale aceluiași set de antrenament, cu scopul de a reduce variația. \n",
    "\n",
    "Procedura de bootstrapping duce la o performanță mai bună a modelului, deoarece scade variația modelului, fără a crește bias-urile. Aceasta înseamnă că, deși iesirile unui singur arbore sunt foarte sensibile la zgomot în setul său de formare, media mai multor arbori nu este, atâta timp cât nu exista corelații. Pur și simplu antrenarea multor arbori pe un singur set de antrenament ar oferi arbori puternic corelați (sau chiar același arbore de multe ori, dacă algoritmul de antrenare este determinist); eșantionarea bootstrap este o modalitate de a corela copacii arătându-le diferite seturi de antrenament.\n",
    "\n",
    "<img src=\"resources/image12.jpg\" width=\"400\">\n",
    "\n",
    "Metoda de mai sus descrie algoritmul de bagaj original pentru arbori. Random Forest diferă într-un singur mod de această schemă generală: folosesc un algoritm de învățare arbore modificat care selectează, la fiecare împărțire a candidatului în procesul de învățare, un subset aleator al caracteristicilor. Acest proces este uneori numit „bagaj de funcții”(feature bagging). Motivul pentru a face acest lucru este corelația arborilor dintr-un eșantion de bootstrap obișnuit: dacă una sau câteva caracteristici sunt predictori foarte puternici pentru variabila de răspuns(ieșire țintă), aceste caracteristici vor fi selectate în multe dintre arborele B, cauzându-le a deveni corelat. O analiză a modului în care bagajul și proiecția aleatorie a subspațiului contribuie la câștigurile de precizie în diferite condiții.\n",
    "\n",
    "Diferite tipuri de modele au avantaje diferite. Modelul Random Forest este foarte bun în tratarea datelor tabulare cu caracteristici numerice sau caracteristici categorice cu mai puțin de sute de categorii. Spre deosebire de modelele liniare, Random Forest este capabil să capteze interacțiuni neliniare între caracteristici și țintă.\n",
    "\n",
    "O notă importantă este că modelele bazate pe arbori nu sunt proiectate să funcționeze cu caracteristici foarte rare. Atunci când avem de a face cu date de intrare reduse(de exemplu, caracteristici categorice cu dimensiuni mari), putem prelucra în prealabil funcțiile rare pentru a genera statistici numerice, sau putem trece la un model liniar, care este mai potrivit pentru astfel de scenarii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie:\n",
    "\n",
    "1. https://medium.com/datadriveninvestor/random-forest-regression-9871bc9a25eb\n",
    "2. https://en.wikipedia.org/wiki/Random_forest\n",
    "3. https://turi.com/learn/userguide/supervised-learning/random_forest_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Epsilon-Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suportul Vector Machine poate fi folosit și ca metodă de regresie, menținând toate caracteristicile principale care caracterizează algoritmul. Deoarece rezultatul este un număr real, devine foarte dificil să prezicem informațiile disponibile, care au posibilități infinite. Însă, ideea principală este întotdeauna aceeași: minimizarea erorilor, individualizarea hiperplanului care maximizează marja. \n",
    "\n",
    "<img src=\"resources/image13.png\" width=\"400\">\n",
    "<img src=\"resources/image14.png\" width=\"400\">\n",
    "\n",
    "- __SVR liniar__\n",
    "<img src=\"resources/image15.png\">\n",
    "- __SVR neliniar:__ Funcțiile nucleului transformă datele într-un spațiu cu caracteristici dimensionale superioare pentru a face posibilă efectuarea separației liniare.\n",
    "<img src=\"resources/image16.png\">\n",
    "<img src=\"resources/image17.png\">\n",
    "Kernel functions:\n",
    "<img src=\"resources/image27.png\">\n",
    "\n",
    "SVR ne oferă flexibilitatea de a defini cât de multă eroare este acceptabilă în modelul nostru și vom găsi o linie adecvată (sau un hiperplan cu dimensiuni mai mari) pentru a se potrivi cu datele.\n",
    "<br>\n",
    "Funcția obiectiv a SVR este de a minimiza coeficienții - mai precis, l norma 2 vectorului coeficient - nu eroarea pătratică. Termenul de eroare este tratat în schimb în constrângerile, unde am stabilit eroarea absolută mai mică sau egală cu o marjă specificată, numită eroare maximă, ϵ (epsilon). Putem regla epsilon pentru a obține exactitatea dorită a modelului nostru. Noile noastre funcții și constrângerile obiective sunt următoarele:\n",
    "<br>\n",
    "__Minimizarea:__\n",
    "<img src=\"resources/image18a.png\" width=\"200\">\n",
    "__constrângeri:__\n",
    "<img src=\"resources/image19.png\" width=\"200\">\n",
    "__Exemplu ilustrativ:__\n",
    "<img src=\"resources/image20.png\" width=\"400\">\n",
    "\n",
    "Pe măsură ce C crește, toleranța noastră pentru puncte în afara de ϵ crește și ea. Pe măsură ce C se apropie de 0, toleranța se apropie de 0, iar ecuația se prăbușește în cea simplificată (deși uneori imposibilă).\n",
    "<br>\n",
    "SVR este un algoritm puternic care ne permite să alegem cât de toleranți suntem de erori, atât printr-o marjă de eroare acceptabilă (ϵ), cât și prin reglarea toleranței noastre de a cădea în afara acelei rate de eroare acceptabile.\n",
    "<br>\n",
    "Deci încercăm să decidem o graniță de decizie la distanță 'e' față de planul hiper original, astfel încât punctele de date cele mai apropiate de planul hiper sau vectorii de suport să se afle în această linie de delimitare.\n",
    "<img src=\"resources/image21.jpeg\" width=\"400\">\n",
    "\n",
    "__De exemplu:__\n",
    "<br>\n",
    "Graficul de mai jos arată rezultatele unui model SVR instruit pe datele Prețurilor locuințelor din Boston. Linia roșie reprezintă linia cea mai potrivită, iar liniile negre reprezintă marja de eroare, ϵ, pe care am setat-o ​​la 5 (5.000 USD).\n",
    "<img src=\"resources/image28.png\" width=\"300\">\n",
    "Dacă setam C=1.0, graficul devine:\n",
    "<img src=\"resources/image29.png\" width=\"300\">\n",
    "Modelul de mai sus pare să se potrivească mult mai bine datelor. Putem merge cu un pas mai departe și căutăm grilă peste C pentru a obține o soluție și mai bună. Să definim o metrică de notare, în cadrul Epsilon . Această măsurătoare măsoară câte puncte din totalul setului nostru de test se încadrează în marja noastră de eroare. Putem monitoriza , de asemenea , modul în care eroarea medie absolută ( MAE ) variază în funcție de C , de asemenea.\n",
    "Mai jos este prezentată o diagramă a rezultatelor căutării grilei, cu valori de C pe axa x și % în Epsilon și MAE în axele y stânga și dreapta.\n",
    "<img src=\"resources/image31.png\" width=\"300\">\n",
    "După cum putem vedea, MAE scade în general pe măsură ce C crește. Cu toate acestea, observăm un maxim în % în metrica Epsilon . Întrucât obiectivul nostru inițial al acestui model a fost maximizarea prezicerii în marja noastră de eroare (5.000 USD), dorim să găsim valoarea lui C care maximizează % în Epsilon . Astfel, C = 6.13.\n",
    "Să construim un ultim model cu hyperparametrele noastre finale, ϵ = 5, C = 6.13.\n",
    "<img src=\"resources/image30.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie:\n",
    "\n",
    "1. https://www.saedsayad.com/support_vector_machine_reg.htm\n",
    "2. https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2\n",
    "3. https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian process regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesele Gaussiene (GP) sunt o metodă generică de învățare supervizată concepută pentru a rezolva problemele de regresie și clasificare probabilistică.\n",
    "Avantajele proceselor gaussiene sunt:\n",
    "<br>\n",
    "~ Predicția interpolează observațiile\n",
    "<br>\n",
    "~ Predicția este probabilistică (gaussiană), astfel încât se poate calcula intervale de încredere empirică și se poate decide pe baza acelora dacă ar trebui să se adapteze predicția în anumite regiuni de interes.\n",
    "<br>\n",
    "~ Versatil: se pot specifica diferite nuclee.\n",
    "<br>\n",
    "Dezavantajele proceselor gaussiene includ:\n",
    "<br>\n",
    "~ Acestea nu sunt rare, adică, folosesc informații complete despre probe/caracteristici pentru a efectua predicția.\n",
    "<br>\n",
    "~ Se pierde eficiența în spațiile dimensionale înalte - și anume atunci când numărul de caracteristici depășește câteva zeci.\n",
    "<br>\n",
    "\n",
    "Regresia proceselor gaussiene (GPR) este o abordare nonparametrică, bayesiană a regresiei care face valuri în zona învățării automate. GPR are mai multe beneficii, funcționând bine pe seturi de date mici și având capacitatea de a furniza măsurători de incertitudine cu privire la previziuni.\n",
    "<br>\n",
    "Abordarea bayesiană are o distribuție a probabilității peste toate valorile posibile. \n",
    "<br>\n",
    "Să presupunem o funcție liniară: __y = wx + ϵ__. Modul în care funcționează abordarea Bayesiană este specificând o distribuție anterioară, p(w) pe parametrul w și relocarea probabilităților bazate pe dovezi (adică date observate) folosind regula lui Bayes:\n",
    "<img src=\"resources/image22.png\" width=\"300\">\n",
    "\n",
    "Distribuția actualizată p (w | y, X) , numită distribuție posterioară , încorporează astfel informații atât din distribuția anterioară, cât și din setul de date. Pentru a obține predicții în puncte de interes nevăzute, x * , distribuția predictivă poate fi calculată prin ponderarea tuturor predicțiilor posibile prin distribuția lor posterioară calculată anterior:\n",
    "<img src=\"resources/image23.png\" width=\"300\">\n",
    "GPR calculează distribuția probabilității pe toate funcțiile admisibile care se potrivesc datelor.\n",
    "<br>\n",
    "Parametrii liberi, numiți hyperparametri, permit o personalizare flexibilă a Procesului Gausian pentru setul de date. Alegerea funcției de covarianță și a hiperparametrii săi se numește selecția modelului.\n",
    "\n",
    "__De exemplu:__\n",
    "<br>\n",
    "Vom începe prin generarea unor date false (de la un model sinusoidal) cu bare de eroare\n",
    "<img src=\"resources/image24.png\" width=\"300\">\n",
    "Acum, vom alege o funcție de kernel (covarianță) care să modeleze aceste date, să presupunem un model mediu zero și să prezicem valorile funcției pe întreaga gamă.\n",
    "<img src=\"resources/image25.png\" width=\"300\">\n",
    "Modelul oferă un handler pentru calcularea probabilității marginalizată a datelor în conformitate cu acest model.\n",
    "Deci, putem folosi acest lucru - combinat cu funcția de minimizare - pentru a se potrivi pentru parametrii de probabilitate maximă. Și rezultă modelul de probabilitate maximă.\n",
    "<img src=\"resources/image26.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografie:\n",
    "\n",
    "1. https://towardsdatascience.com/quick-start-to-gaussian-process-regression-36d838810319\n",
    "2. https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-51.pdf\n",
    "3. https://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "4. https://george.readthedocs.io/en/latest/tutorials/first/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
